{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm # loops show smart progress meter\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Player(Enum):\n",
    "#    NOUGHTS = -1\n",
    "#    CROSSES = 1\n",
    "\n",
    "class Board:\n",
    "    \"\"\" holds a 3x3 board.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def coordFromLinearCoord(pos):\n",
    "        assert(0 <= pos < 9)\n",
    "        \n",
    "        x = pos % 3\n",
    "        y = int(pos/3)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def coordToLinearCoord(x, y):\n",
    "        assert(0 <= x < 3)\n",
    "        assert(0 <= y < 3)\n",
    "        \n",
    "        return x + 3 * y\n",
    "    \n",
    "    def __init__(self, bInit):\n",
    "        \"\"\" bInit is a numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(bInit.shape == ((3,3)))\n",
    "        \n",
    "        self.b = bInit\n",
    "    \n",
    "    def hasEmptyCell(self):\n",
    "        return np.any(self.b == 0) == False\n",
    "    \n",
    "    def isEmptyCell(self, x, y):\n",
    "        return self.b[x, y] == 0\n",
    "    \n",
    "    def check(self, player):\n",
    "        \"\"\"\n",
    "        This function returns player if this player won, 2 if draw, and 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(player in [-1, 1])\n",
    "        \n",
    "        # check horizontal and vertical lines for win.\n",
    "        \n",
    "        for i in range(3):\n",
    "            if (self.b[i,0] == player and self.b[i,0] == self.b[i,1] == self.b[i,2]) or \\\n",
    "               (self.b[0,i] == player and self.b[0,i] == self.b[1,i] == self.b[2,i]):\n",
    "                return player\n",
    "\n",
    "        # check diagonals for win.\n",
    "        \n",
    "        if (self.b[0,0] == player and self.b[0,0] == self.b[1,1] == self.b[2,2]) or \\\n",
    "           (self.b[0,2] == player and self.b[0,2] == self.b[1,1] == self.b[2,0]):\n",
    "            return player\n",
    "        \n",
    "        \n",
    "        if self.hasEmptyCell():   # draw.\n",
    "            return 2\n",
    "        \n",
    "        return 0    # game not over.\n",
    "\n",
    "    def toKey(self):\n",
    "        \n",
    "        s = 0\n",
    "        i = 0\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                s += (self.b[x,y] + 2) * 10**i\n",
    "                i += 1\n",
    "\n",
    "        return int(s)\n",
    "\n",
    "    def fromKey(self, i):\n",
    "        \n",
    "        self.b = np.zeros((3, 3))\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                self.b[x,y] = i%10 - 2\n",
    "                i = int(i/10)\n",
    "                \n",
    "    def move(self, x, y, player):\n",
    "        self.b[x,y] = player\n",
    "        \n",
    "        return self.check(player)\n",
    "    \n",
    "    def possibleActions(self):\n",
    "        \n",
    "        \"\"\" returns list of empty cells using linear coords.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                if self.isEmptyCell(x, y):\n",
    "                    actions.append(Board.coordToLinearCoord(x,y))\n",
    "        \n",
    "        return actions\n",
    "\n",
    "def getAllStartStates():\n",
    "    def f(keys, b, i):\n",
    "        # i is initial cell in linear coords.\n",
    "        \n",
    "        assert(0 <= i < 9)\n",
    "        \n",
    "        x, y = Board.coordFromLinearCoord(i)\n",
    "        \n",
    "        for p in [-1, 0, 1]:\n",
    "            b.b[x,y] = p\n",
    "            \n",
    "            if np.sum(b.b == 1) - np.sum(b.b == -1) in [0, 1] and b.check(1) == 0 and b.check(-1) == 0:\n",
    "                keys.append(b.toKey())\n",
    "            \n",
    "            if i < 8:\n",
    "                f(keys, b, i+1)\n",
    "    \n",
    "    board = Board(np.zeros((3,3)))\n",
    "    boardKeys = [board.toKey()]\n",
    "    \n",
    "    f(boardKeys, board, 0)\n",
    "    \n",
    "    return boardKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyRandomMove(board, pure = False):\n",
    "    \n",
    "    p = np.where(board.b == 0, 1, 0) # p is 3x3 grid which is 1 where we have empty cells\n",
    "    \n",
    "    if not pure:\n",
    "        if p[1,1] == 1:  # (1, 1) is empty\n",
    "            p[1,1] = 3\n",
    "        for x, y in [[0,0],[0,2],[2,0],[2,2]]:  # the four corners\n",
    "            if p[x,y] == 1:\n",
    "                p[x,y] = 2\n",
    "    \n",
    "    policy = p / np.sum(p)\n",
    "    \n",
    "    return chooseMove(policy)\n",
    "\n",
    "# does this function always return something?\n",
    "\n",
    "def chooseMove(policy):\n",
    "    \n",
    "    u = np.random.uniform()\n",
    "    c = 0\n",
    "    \n",
    "    for x in [0, 1, 2]:\n",
    "        for y in [0, 1, 2]:\n",
    "            c += policy[x, y]\n",
    "            if u < c:\n",
    "                return x, y\n",
    "\n",
    "def policyMCMove(board, q, player):\n",
    "    \"\"\"\n",
    "    We choose the action that has the highest current q-value.\n",
    "    \"\"\"\n",
    "    key = board.toKey()\n",
    "    actions = board.possibleActions()  # actually just empty cells.\n",
    "\n",
    "    qValues = [q[key, action] for action in actions]\n",
    "\n",
    "    bestAction = actions[qValues.index(max(qValues))]\n",
    "    \n",
    "    return Board.coordFromLinearCoord(bestAction)\n",
    "\n",
    "def policyMCMoveEpsGreedy(board, q, player, epsilon):\n",
    "    \"\"\"\n",
    "    With probabiliy epsilon we choose an action randomly.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        a = random.choice(board.possibleActions())\n",
    "        return Board.coordFromLinearCoord(a)\n",
    "    \n",
    "    return policyMCMove(board, q, player)\n",
    "\n",
    "def expectedTargetUnderEpsGreedy(board, q, epsilon):\n",
    "\n",
    "    key = board.toKey()\n",
    "    \n",
    "    qValues = [q[(key, a)] for a in board.possibleActions()]\n",
    "    bestq = max(qValues)\n",
    "    \n",
    "    return bestq * (1 - epsilon) + epsilon * sum(qValues) / len(actions)\n",
    "\n",
    "def playSingleGame(epsilon):\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    board = Board(np.zeros((3,3)))\n",
    "    \n",
    "    listStateAction = []\n",
    "    \n",
    "    gameState = 0\n",
    "    \n",
    "    while gameState == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(board)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(board, q, player, epsilon)\n",
    "            listStateAction.append((board.toKey(), (x, y)))\n",
    "        \n",
    "        gameState = board.move(x, y, player)\n",
    "        player *= -1\n",
    "        \n",
    "    return gameState, listStateAction\n",
    "\n",
    "def playGame(nGames, q, epsilon = -1):\n",
    "    \n",
    "    win1 = 0\n",
    "    winM1 = 0\n",
    "    draw = 0\n",
    "    \n",
    "    for _ in tqdm(range(nGames)):\n",
    "\n",
    "        gameOver, _ = playSingleGame(epsilon)\n",
    "\n",
    "        if gameOver == 2:\n",
    "            draw += 1\n",
    "        elif gameOver == 1:\n",
    "            win1 += 1\n",
    "        else:\n",
    "            winM1 += 1\n",
    "\n",
    "\n",
    "    print(win1/nGames, winM1/nGames, draw/nGames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(keys):\n",
    "\n",
    "    q = {}\n",
    "    n = {}\n",
    "\n",
    "    for key in keys:\n",
    "        board = Board(np.zeros((3,3)))\n",
    "        board.fromKey(key)\n",
    "              \n",
    "        for action in board.possibleActions():\n",
    "            q[(key, action)] = 0.5\n",
    "            n[(key, action)] = 0\n",
    "    \n",
    "    return q, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the $\\epsilon$-greedy method.  This means that we choose the current best method most of the time but sometimes, namely with probability $\\epsilon$, we explore.  In our case, one player plays randomly and the other follows this strategy.  The action-value function $q(s, a)$ is a function of state $s$ and action $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playSingleGame(epsilon):\n",
    "    \n",
    "    player = random.choice([-1, 1])\n",
    "    board = Board(np.zeros((3, 3)))\n",
    "    \n",
    "    listStateAction = []\n",
    "    \n",
    "    gameState = 0\n",
    "    \n",
    "    while gameState == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(board)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(board, q, player, epsilon)\n",
    "            listStateAction.append((board.toKey(), (x, y)))\n",
    "        \n",
    "        gameState = board.move(x, y, player)\n",
    "        player *= -1\n",
    "        \n",
    "    return gameState, listStateAction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [01:17<00:00, 1295.85it/s]\n",
      "100%|██████████| 100000/100000 [01:19<00:00, 1252.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.066 0.8715 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#epsilon greedy\n",
    "\n",
    "q, n = initialise(getAllStartStates())\n",
    "\n",
    "epsilon = 0.05\n",
    "nGames = 100000\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "\n",
    "    finalGameState, listStateAction = playSingleGame(epsilon)\n",
    "\n",
    "    # r is the reward.\n",
    "    \n",
    "    if finalGameState == 2: # draw\n",
    "        r = 0.5\n",
    "    elif finalGameState == 1: # loss\n",
    "        r = 0\n",
    "    else: # win\n",
    "        r = 1\n",
    "\n",
    "    for key, (x, y) in listStateAction:\n",
    "        action = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, action)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, n = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring starts\n",
    "nGames = 100000\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    \n",
    "    player = -1\n",
    "    \n",
    "    randomAction = True\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    listStateAction=[]\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            key = b.toKey()\n",
    "            if randomAction:\n",
    "                p = np.where(b.b==0, 1, 0)\n",
    "                policy = p / np.sum(p)\n",
    "                x,y = chooseMove(policy)\n",
    "                randomAction = False\n",
    "            else:\n",
    "                x,y = policyMCMove(b, q, player)\n",
    "            \n",
    "            listStateAction.append((key, (x, y)))\n",
    "        \n",
    "        gameOver = b.move(x, y, player)\n",
    "        \n",
    "        player *= -1\n",
    "        \n",
    "    if gameOver == 2:\n",
    "        r = 0.5\n",
    "    elif gameOver == 1:\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 1\n",
    "    \n",
    "    for key, (x,y) in listStateAction:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, a)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, c = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#off policy\n",
    "nGames = 1000000\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    listStateAction=[]\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x,y=policyRandomMove(b, True)\n",
    "            listStateAction.append((b.toKey(), (x, y)))\n",
    "        gameOver = b.move(x,y,player)\n",
    "        player=-player\n",
    "        \n",
    "    if gameOver==2:\n",
    "        r=0.5\n",
    "    elif gameOver==1:\n",
    "        r=0\n",
    "    else:\n",
    "        r=1\n",
    "\n",
    "    W=1\n",
    "    for k, (x,y) in listStateAction[::]:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        c[(k,a)]=c[(k,a)]+W\n",
    "        q[(k,a)]=q[(k,a)]+W*(r-q[(k,a)])/c[(k,a)]\n",
    "        b = Board(np.zeros((3,3)))\n",
    "        b.fromKey(k)\n",
    "        pA=b.possibleActions()\n",
    "        bestA=pA[0]\n",
    "        for aa in pA[1:]:\n",
    "            if q[(k,aa)]>q[(k,bestA)]:\n",
    "                bestA=aa\n",
    "        if bestA!=a:\n",
    "            break\n",
    "        W=W*len(pA)\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sarsa\n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "epsilon = 0.1\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,0.05)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = x+3*y\n",
    "                newKey = b.toKey()\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(q[(newKey,newAction)]-q[(key,action)])\n",
    "        if player==-1 and newAction is not None:\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q learning\n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,0.1)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = x+3*y\n",
    "                newKey = b.toKey()\n",
    "                pA=b.possibleActions()\n",
    "                maxq=q[(newKey,pA[0])]\n",
    "                for aa in pA[1:]:\n",
    "                    if q[(newKey,aa)]>maxq:\n",
    "                        maxq=q[(newKey,aa)]\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(maxq-q[(key,action)])\n",
    "        if player==-1 and newAction is not None:\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected sarsa where behaviour policy is eps greedy derived from q\n",
    "# (expected sarsa where behaviour policy is greedy derived from q = q learning)\n",
    "nGames = 100000\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    action = None\n",
    "    newAction = None\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b, q, player, epsilon)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = Board.coordToLinearCoord(x, y)\n",
    "                newKey = b.toKey()\n",
    "                expectedTarget = expectedTargetUnderEpsGreedy(b, q, epsilon)\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(expectedTarget-q[(key,action)])\n",
    "        if player==-1 and newAction is not None:\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
