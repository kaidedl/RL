{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm # loops show smart progress meter\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Board:\n",
    "    \"\"\" holds a 3x3 board.\n",
    "    \"\"\"\n",
    "        \n",
    "    @staticmethod\n",
    "    def coordFromLinearCoord(pos):\n",
    "        assert(0 <= pos < 9)\n",
    "        \n",
    "        x = pos % 3\n",
    "        y = int(pos/3)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def coordToLinearCoord(x, y):\n",
    "        assert(0 <= x < 3)\n",
    "        assert(0 <= y < 3)\n",
    "        \n",
    "        return x + 3 * y\n",
    "    \n",
    "    def __init__(self, b = np.zeros((3,3))):\n",
    "        \"\"\" b is a numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(b.shape == ((3,3)))\n",
    "        \n",
    "        self.b = b\n",
    "    \n",
    "    def hasEmptyCell(self):\n",
    "        return np.any(self.b == 0) == False\n",
    "    \n",
    "    def isEmptyCell(self, x, y):\n",
    "        return self.b[x, y] == 0\n",
    "    \n",
    "    def check(self, player):\n",
    "        \"\"\"\n",
    "        This function returns player if this player won, 2 if draw, and 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(player in [-1, 1])\n",
    "                \n",
    "        b = self.b\n",
    "        \n",
    "        # check horizontal and vertical lines for win.\n",
    "\n",
    "        for i in range(3):\n",
    "            if (b[i,0] == player and b[i,0] == b[i,1] == b[i,2]) or \\\n",
    "               (b[0,i] == player and b[0,i] == b[1,i] == b[2,i]):\n",
    "                return player\n",
    "\n",
    "        # check diagonals for win.\n",
    "        \n",
    "        if (b[0,0] == player and b[0,0] == b[1,1] == b[2,2]) or \\\n",
    "           (b[0,2] == player and b[0,2] == b[1,1] == b[2,0]):\n",
    "            return player\n",
    "        \n",
    "        \n",
    "        if self.hasEmptyCell():   # draw.\n",
    "            return 2\n",
    "        \n",
    "        return 0    # game not over.\n",
    "\n",
    "    def toKey(self):\n",
    "        \n",
    "        s = 0\n",
    "        i = 0\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                s += (self.b[x,y] + 2) * 10**i\n",
    "                i += 1\n",
    "\n",
    "        return int(s)\n",
    "\n",
    "    def fromKey(self, i):\n",
    "        \n",
    "        self.b = np.zeros((3, 3))\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                self.b[x,y] = i % 10 - 2\n",
    "                i = int(i/10)\n",
    "                \n",
    "    def move(self, x, y, player):\n",
    "        self.b[x,y] = player\n",
    "        \n",
    "        return self.check(player)\n",
    "    \n",
    "    def possibleActions(self):\n",
    "        \n",
    "        \"\"\" returns list of empty cells using linear coords.\n",
    "        \"\"\"\n",
    "        \n",
    "        emptyCells = [(x, y) for x in range(3) for y in range(3) if self.isEmptyCell(x, y)]\n",
    "\n",
    "        return [Board.coordToLinearCoord(x,y) for (x, y) in emptyCells]\n",
    "\n",
    "def getAllStartStates():\n",
    "    def getStartStates(keys, b, pos):\n",
    "        # i is initial cell in linear coords.\n",
    "        \n",
    "        assert(0 <= pos < 9)\n",
    "        \n",
    "        x, y = Board.coordFromLinearCoord(pos)\n",
    "        \n",
    "        for p in [-1, 0, 1]:\n",
    "            b.b[x,y] = p\n",
    "            \n",
    "            if np.sum(b.b == 1) - np.sum(b.b == -1) in [0, 1] and b.check(1) == 0 and b.check(-1) == 0:\n",
    "                keys.append(b.toKey())\n",
    "            \n",
    "            if pos < 8:\n",
    "                getStartStates(keys, b, pos + 1)\n",
    "    \n",
    "    board = Board()\n",
    "    boardKeys = [board.toKey()]\n",
    "    \n",
    "    getStartStates(boardKeys, board, 0)\n",
    "    \n",
    "    return boardKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyRandomMove(board, pure = False):\n",
    "    \n",
    "    p = np.where(board.b == 0, 1, 0) # p is 3x3 grid which is 1 where we have empty cells\n",
    "    \n",
    "    corners = [[0,0], [0,2], [2,0], [2,2]]\n",
    "    \n",
    "    if not pure:\n",
    "        if p[1,1] == 1:  # (1, 1) is empty\n",
    "            p[1,1] = 3\n",
    "        for x, y in corners:\n",
    "            if p[x,y] == 1:\n",
    "                p[x,y] = 2\n",
    "    \n",
    "    policy = p / np.sum(p)\n",
    "    \n",
    "    return chooseMove(policy)\n",
    "\n",
    "# does this function always return something?\n",
    "\n",
    "def chooseMove(policy):\n",
    "    \n",
    "    u = np.random.uniform()\n",
    "    c = 0\n",
    "\n",
    "    for x in range(3):\n",
    "        for y in range(3):\n",
    "            c += policy[x, y]\n",
    "            if u < c:\n",
    "                return x, y\n",
    "\n",
    "def policyMCMove(board, q, player):\n",
    "    \"\"\"\n",
    "    We choose the action that has the highest current q-value.\n",
    "    \"\"\"\n",
    "    key = board.toKey()\n",
    "    actions = board.possibleActions()  # actually just empty cells.\n",
    "\n",
    "    qValues = [q[key, action] for action in actions]\n",
    "\n",
    "    bestAction = actions[qValues.index(max(qValues))]\n",
    "    \n",
    "    return Board.coordFromLinearCoord(bestAction)\n",
    "\n",
    "def policyMCMoveEpsGreedy(board, q, player, epsilon):\n",
    "    \"\"\"\n",
    "    With probabiliy epsilon we choose an action randomly.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        a = random.choice(board.possibleActions())\n",
    "        return Board.coordFromLinearCoord(a)\n",
    "    \n",
    "    return policyMCMove(board, q, player)\n",
    "\n",
    "def expectedTargetUnderEpsGreedy(board, q, epsilon):\n",
    "\n",
    "    key = board.toKey()\n",
    "    \n",
    "    qValues = [q[(key, a)] for a in board.possibleActions()]\n",
    "    bestq = max(qValues)\n",
    "    \n",
    "    return bestq * (1 - epsilon) + epsilon * sum(qValues) / len(actions)\n",
    "\n",
    "def playSingleGame(epsilon, returnStateActionPairs = True):\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    board = Board(np.zeros((3,3)))\n",
    "    \n",
    "    listStateAction = []\n",
    "    \n",
    "    gameState = 0\n",
    "    \n",
    "    while gameState == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(board)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(board, q, player, epsilon)\n",
    "            if returnStateActionPairs:\n",
    "                listStateAction.append((board.toKey(), (x, y)))\n",
    "        \n",
    "        gameState = board.move(x, y, player)\n",
    "        player *= -1\n",
    "        \n",
    "    return gameState, listStateAction\n",
    "\n",
    "def playGame(nGames, q, epsilon = -1):\n",
    "    \n",
    "    win1 = 0\n",
    "    winM1 = 0\n",
    "    draw = 0\n",
    "    \n",
    "    for _ in tqdm(range(nGames)):\n",
    "\n",
    "        gameOver, _ = playSingleGame(epsilon, False)\n",
    "\n",
    "        if gameOver == 2:\n",
    "            draw += 1\n",
    "        elif gameOver == 1:\n",
    "            win1 += 1\n",
    "        else:\n",
    "            winM1 += 1\n",
    "\n",
    "    print(win1/nGames, winM1/nGames, draw/nGames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(keys):\n",
    "\n",
    "    q = {}\n",
    "    n = {}\n",
    "\n",
    "    for key in keys:\n",
    "        board = Board(np.zeros((3,3)))\n",
    "        board.fromKey(key)\n",
    "              \n",
    "        for action in board.possibleActions():\n",
    "            q[(key, action)] = 0.5\n",
    "            n[(key, action)] = 0\n",
    "    \n",
    "    return q, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the $\\epsilon$-greedy method.  This means that we choose the current best action most of the time but sometimes, namely with probability $\\epsilon$, we explore.  In our case, one player plays randomly and the other follows this strategy.  The action-value function $q(s, a)$ is a function of state $s$ and action $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14253/100000 [00:14<01:25, 998.60it/s] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-9c17e4b31f74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnGames\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[0mfinalGameState\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlistStateAction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplaySingleGame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;31m# r is the reward.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-27-ef26655e965e>\u001b[0m in \u001b[0;36mplaySingleGame\u001b[1;34m(epsilon, returnStateActionPairs)\u001b[0m\n\u001b[0;32m     78\u001b[0m                 \u001b[0mlistStateAction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoKey\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 80\u001b[1;33m         \u001b[0mgameState\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mboard\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     81\u001b[0m         \u001b[0mplayer\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-f81a4e0b601f>\u001b[0m in \u001b[0;36mmove\u001b[1;34m(self, x, y, player)\u001b[0m\n\u001b[0;32m     85\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mplayer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpossibleActions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-f81a4e0b601f>\u001b[0m in \u001b[0;36mcheck\u001b[1;34m(self, player)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhasEmptyCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m   \u001b[1;31m# draw.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-26-f81a4e0b601f>\u001b[0m in \u001b[0;36mhasEmptyCell\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mhasEmptyCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mb\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0misEmptyCell\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36many\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\Lib\\site-packages\\numpy\\core\\fromnumeric.py\u001b[0m in \u001b[0;36many\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   2315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2316\u001b[0m     \"\"\"\n\u001b[1;32m-> 2317\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_wrapreduction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_or\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'any'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeepdims\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2318\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2319\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#epsilon greedy\n",
    "\n",
    "q, n = initialise(getAllStartStates())\n",
    "\n",
    "epsilon = 0.05\n",
    "nGames = 100000\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "\n",
    "    finalGameState, listStateAction = playSingleGame(epsilon)\n",
    "\n",
    "    # r is the reward.\n",
    "    \n",
    "    if finalGameState == 2: # draw\n",
    "        r = 0.5\n",
    "    elif finalGameState == 1: # loss\n",
    "        r = 0\n",
    "    else: # win\n",
    "        r = 1\n",
    "\n",
    "    for key, (x, y) in listStateAction:\n",
    "        action = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, action)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, n = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exploring starts\n",
    "nGames = 100000\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    \n",
    "    player = -1\n",
    "    \n",
    "    randomAction = True\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    listStateAction=[]\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            key = b.toKey()\n",
    "            if randomAction:\n",
    "                p = np.where(b.b==0, 1, 0)\n",
    "                policy = p / np.sum(p)\n",
    "                x,y = chooseMove(policy)\n",
    "                randomAction = False\n",
    "            else:\n",
    "                x,y = policyMCMove(b, q, player)\n",
    "            \n",
    "            listStateAction.append((key, (x, y)))\n",
    "        \n",
    "        gameOver = b.move(x, y, player)\n",
    "        \n",
    "        player *= -1\n",
    "        \n",
    "    if gameOver == 2:\n",
    "        r = 0.5\n",
    "    elif gameOver == 1:\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 1\n",
    "    \n",
    "    for key, (x,y) in listStateAction:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, a)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, c = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#off policy\n",
    "nGames = 1000000\n",
    "\n",
    "def playSingleGame(epsilon, returnStateActionPairs = True):\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    board = Board(np.zeros((3,3)))\n",
    "    \n",
    "    listStateAction = []\n",
    "    \n",
    "    gameState = 0\n",
    "    \n",
    "    while gameState == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(board)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(board, q, player, epsilon)\n",
    "            if returnStateActionPairs:\n",
    "                listStateAction.append((board.toKey(), (x, y)))\n",
    "        \n",
    "        gameState = board.move(x, y, player)\n",
    "        player *= -1\n",
    "        \n",
    "    return gameState, listStateAction\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver = 0\n",
    "    listStateAction=[]\n",
    "    \n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x,y=policyRandomMove(b, True)\n",
    "            listStateAction.append((b.toKey(), (x, y)))\n",
    "        gameOver = b.move(x,y,player)\n",
    "        player=-player\n",
    "        \n",
    "    if gameOver==2:\n",
    "        r=0.5\n",
    "    elif gameOver==1:\n",
    "        r=0\n",
    "    else:\n",
    "        r=1\n",
    "\n",
    "    W=1\n",
    "    for k, (x,y) in listStateAction[::]:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        c[(k,a)]=c[(k,a)]+W\n",
    "        q[(k,a)]=q[(k,a)]+W*(r-q[(k,a)])/c[(k,a)]\n",
    "        b = Board(np.zeros((3,3)))\n",
    "        b.fromKey(k)\n",
    "        pA=b.possibleActions()\n",
    "        bestA=pA[0]\n",
    "        for aa in pA[1:]:\n",
    "            if q[(k,aa)]>q[(k,bestA)]:\n",
    "                bestA=aa\n",
    "        if bestA!=a:\n",
    "            break\n",
    "        W=W*len(pA)\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sarsa\n",
    "nGames = 100000\n",
    "alpha = 0.1\n",
    "epsilon = 0.05\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    \n",
    "    gameOver = 0\n",
    "    action = None\n",
    "    newAction = None\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b, q, player, epsilon)\n",
    "            \n",
    "            if action is None:\n",
    "                action = Board.coordToLinearCoord(x, y)\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = Board.coordToLinearCoord(x, y)\n",
    "                newKey = b.toKey()\n",
    "                \n",
    "        gameOver = b.move(x,y,player)\n",
    "        \n",
    "        u = (key, action)\n",
    "        \n",
    "        if gameOver == -1:\n",
    "            q[u] = q[u] + alpha * (1 - q[u])\n",
    "        elif gameOver == 1:\n",
    "            q[u] = q[u] + alpha * (0 - q[u])\n",
    "        elif newAction is not None:\n",
    "            q[u] = q[u] + alpha * (q[(newKey,newAction)] - q[u])\n",
    "        if player == -1 and newAction is not None:\n",
    "            action = newAction\n",
    "            key = newKey\n",
    "        \n",
    "        player *= -1\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#q learning\n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,0.1)\n",
    "            if action is None:\n",
    "                action = Board.coordToLinearCoord(x, y)\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = Board.coordToLinearCoord(x, y)\n",
    "                newKey = b.toKey()\n",
    "                pA=b.possibleActions()\n",
    "                maxq=q[(newKey,pA[0])]\n",
    "                for aa in pA[1:]:\n",
    "                    if q[(newKey,aa)]>maxq:\n",
    "                        maxq=q[(newKey,aa)]\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(maxq-q[(key,action)])\n",
    "        if player==-1 and newAction is not None:\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expected sarsa where behaviour policy is eps greedy derived from q\n",
    "# (expected sarsa where behaviour policy is greedy derived from q = q learning)\n",
    "nGames = 100000\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    action = None\n",
    "    newAction = None\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b, q, player, epsilon)\n",
    "            if action is None:\n",
    "                action = Board.coordToLinearCoord(x, y)\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = Board.coordToLinearCoord(x, y)\n",
    "                newKey = b.toKey()\n",
    "                expectedTarget = expectedTargetUnderEpsGreedy(b, q, epsilon)\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(expectedTarget-q[(key,action)])\n",
    "        if player==-1 and newAction is not None:\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
