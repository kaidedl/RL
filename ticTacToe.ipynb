{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm # loops show smart progress meter\n",
    "from enum import Enum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#class Player(Enum):\n",
    "#    NOUGHTS = -1\n",
    "#    CROSSES = 1\n",
    "\n",
    "class Board:\n",
    "    \"\"\" holds a 3x3 board.\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def coordFromLinearCoord(pos):\n",
    "        assert(0 <= pos < 9)\n",
    "        \n",
    "        x = pos % 3\n",
    "        y = int(pos/3)\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "    @staticmethod\n",
    "    def coordToLinearCoord(x, y):\n",
    "        assert(0 <= x < 3)\n",
    "        assert(0 <= y < 3)\n",
    "        \n",
    "        return x + 3 * y\n",
    "    \n",
    "    def __init__(self, bInit):\n",
    "        \"\"\" bInit is a numpy array.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(bInit.shape == ((3,3)))\n",
    "        \n",
    "        self.b = bInit\n",
    "    \n",
    "    def hasEmptyCell(self):\n",
    "        return np.any(self.b == 0) == False\n",
    "    \n",
    "    def isEmptyCell(self, x, y):\n",
    "        return self.b[x, y] == 0\n",
    "    \n",
    "    def check(self, player):\n",
    "        \"\"\"\n",
    "        This function returns player if this player won, 2 if draw, and 0 otherwise.\n",
    "        \"\"\"\n",
    "        \n",
    "        assert(player in [-1, 1])\n",
    "        \n",
    "        # check horizontal and vertical lines for win.\n",
    "        \n",
    "        for i in range(3):\n",
    "            if (self.b[i,0] == player and self.b[i,0] == self.b[i,1] == self.b[i,2]) or \\\n",
    "               (self.b[0,i] == player and self.b[0,i] == self.b[1,i] == self.b[2,i]):\n",
    "                return player\n",
    "\n",
    "        # check diagonals for win.\n",
    "        \n",
    "        if (self.b[0,0] == player and self.b[0,0] == self.b[1,1] == self.b[2,2]) or \\\n",
    "           (self.b[0,2] == player and self.b[0,2] == self.b[1,1] == self.b[2,0]):\n",
    "            return player\n",
    "        \n",
    "        \n",
    "        if self.hasEmptyCell():   # draw.\n",
    "            return 2\n",
    "        \n",
    "        return 0    # game not over.\n",
    "\n",
    "    def toKey(self):\n",
    "        \n",
    "        s = 0\n",
    "        i = 0\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                s += (self.b[x,y] + 2) * 10**i\n",
    "                i += 1\n",
    "\n",
    "        return int(s)\n",
    "\n",
    "    def fromKey(self, i):\n",
    "        \n",
    "        self.b = np.zeros((3, 3))\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                self.b[x,y] = i%10 - 2\n",
    "                i = int(i/10)\n",
    "                \n",
    "    def move(self, x, y, player):\n",
    "        self.b[x,y] = player\n",
    "        \n",
    "        return self.check(player)\n",
    "    \n",
    "    def possibleActions(self):\n",
    "        \n",
    "        \"\"\" returns list of empty cells using linear coords.\n",
    "        \"\"\"\n",
    "        \n",
    "        actions = []\n",
    "        \n",
    "        for x in range(3):\n",
    "            for y in range(3):\n",
    "                if self.isEmptyCell(x, y):\n",
    "                    actions.append(Board.coordToLinearCoord(x,y))\n",
    "        \n",
    "        return actions\n",
    "\n",
    "def getAllStartStates():\n",
    "    def f(keys, b, i):\n",
    "        # i is initial cell in linear coords.\n",
    "        \n",
    "        assert(0 <= i < 9)\n",
    "        \n",
    "        x, y = Board.coordFromLinearCoord(i)\n",
    "        \n",
    "        for p in [-1, 0, 1]:\n",
    "            b.b[x,y] = p\n",
    "            \n",
    "            if np.sum(b.b == 1) - np.sum(b.b == -1) in [0, 1] and b.check(1) == 0 and b.check(-1) == 0:\n",
    "                keys.append(b.toKey())\n",
    "            \n",
    "            if i < 8:\n",
    "                f(keys, b, i+1)\n",
    "    \n",
    "    board = Board(np.zeros((3,3)))\n",
    "    boardKeys = [board.toKey()]\n",
    "    \n",
    "    f(boardKeys, board, 0)\n",
    "    \n",
    "    return boardKeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policyRandomMove(board, pure = False):\n",
    "    \n",
    "    p = np.where(board.b == 0, 1, 0) # p is 3x3 grid which is 1 where we have empty cells\n",
    "    \n",
    "    if not pure:\n",
    "        if p[1,1] == 1:  # (1, 1) is empty\n",
    "            p[1,1] = 3\n",
    "        for x, y in [[0,0],[0,2],[2,0],[2,2]]:  # the four corners\n",
    "            if p[x,y] == 1:\n",
    "                p[x,y] = 2\n",
    "    \n",
    "    policy = p / np.sum(p)\n",
    "    \n",
    "    return chooseMove(policy)\n",
    "\n",
    "# does this function always return something?\n",
    "\n",
    "def chooseMove(policy):\n",
    "    \n",
    "    u = np.random.uniform()\n",
    "    c = 0\n",
    "    \n",
    "    for x in [0, 1, 2]:\n",
    "        for y in [0, 1, 2]:\n",
    "            c += policy[x, y]\n",
    "            if u < c:\n",
    "                return x, y\n",
    "\n",
    "def policyMCMove(board, q, player):\n",
    "    \"\"\"\n",
    "    We choose the action that has the highest current q-value.\n",
    "    \"\"\"\n",
    "    key = board.toKey()\n",
    "    actions = board.possibleActions()  # actually just empty cells.\n",
    "\n",
    "    qValues = [q[key, action] for action in actions]\n",
    "\n",
    "    bestAction = actions[qValues.index(max(qValues))]\n",
    "    \n",
    "    return Board.coordFromLinearCoord(bestAction)\n",
    "\n",
    "def policyMCMoveEpsGreedy(board, q, player, epsilon):\n",
    "    \"\"\"\n",
    "    With probabiliy epsilon we choose an action randomly.\n",
    "    \"\"\"\n",
    "    if random.random() < epsilon:\n",
    "        a = random.choice(board.possibleActions())\n",
    "        return Board.coordFromLinearCoord(a)\n",
    "    \n",
    "    return policyMCMove(board, q, player)\n",
    "\n",
    "def expectedTargetUnderEpsGreedy(board, q, epsilon):\n",
    "\n",
    "    key = board.toKey()\n",
    "    actions = board.possibleActions()\n",
    "    \n",
    "    qValues = [q[(key, a)] for a in actions]\n",
    "    bestq = max(qValues)\n",
    "    \n",
    "    return bestq * (1 - epsilon) + epsilon * sum(qValues) / len(actions)\n",
    "\n",
    "def playSingleGame(epsilon, returnStateActionPairs = True):\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    board = Board(np.zeros((3,3)))\n",
    "    \n",
    "    listStateAction = []\n",
    "    \n",
    "    gameState = 0\n",
    "    \n",
    "    while gameState == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(board)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(board, q, player, epsilon)\n",
    "            if returnStateActionPairs:\n",
    "                listStateAction.append((board.toKey(), (x, y)))\n",
    "        \n",
    "        gameState = board.move(x, y, player)\n",
    "        player *= -1\n",
    "        \n",
    "    return gameState, listStateAction\n",
    "\n",
    "def playGame(nGames, q, epsilon = -1):\n",
    "    \n",
    "    win1 = 0\n",
    "    winM1 = 0\n",
    "    draw = 0\n",
    "    \n",
    "    for _ in tqdm(range(nGames)):\n",
    "\n",
    "        gameOver, _ = playSingleGame(epsilon, False)\n",
    "\n",
    "        if gameOver == 2:\n",
    "            draw += 1\n",
    "        elif gameOver == 1:\n",
    "            win1 += 1\n",
    "        else:\n",
    "            winM1 += 1\n",
    "\n",
    "    print(win1/nGames, winM1/nGames, draw/nGames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialise(keys):\n",
    "\n",
    "    q = {}\n",
    "    n = {}\n",
    "\n",
    "    for key in keys:\n",
    "        board = Board(np.zeros((3,3)))\n",
    "        board.fromKey(key)\n",
    "              \n",
    "        for action in board.possibleActions():\n",
    "            q[(key, action)] = 0.5\n",
    "            n[(key, action)] = 0\n",
    "    \n",
    "    return q, n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we look at the $\\epsilon$-greedy method.  This means that we choose the current best method most of the time but sometimes, namely with probability $\\epsilon$, we explore.  In our case, one player plays randomly and the other follows this strategy.  The action-value function $q(s, a)$ is a function of state $s$ and action $a$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:30<00:00, 3288.91it/s]\n",
      "100%|██████████| 100000/100000 [00:25<00:00, 3889.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05351 0.8658 0.08069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MC epsilon greedy\n",
    "\n",
    "q, n = initialise(getAllStartStates())\n",
    "\n",
    "epsilon = 0.05\n",
    "nGames = 100000\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "\n",
    "    finalGameState, listStateAction = playSingleGame(epsilon)\n",
    "\n",
    "    # r is the reward.\n",
    "    \n",
    "    if finalGameState == 2: # draw\n",
    "        r = 0.5\n",
    "    elif finalGameState == 1: # loss\n",
    "        r = 0\n",
    "    else: # win\n",
    "        r = 1\n",
    "\n",
    "    for key, (x, y) in listStateAction:\n",
    "        action = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, action)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = getAllStartStates()\n",
    "q, n = initialise(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 200000/200000 [00:25<00:00, 7916.11it/s]\n",
      "100%|██████████| 200000/200000 [00:56<00:00, 3511.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08122 0.85802 0.06076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#MC exploring starts\n",
    "nGames = 200000\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    b.fromKey(random.choice(s))\n",
    "    \n",
    "    player = -1\n",
    "    \n",
    "    randomAction = True\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    listStateAction=[]\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            key = b.toKey()\n",
    "            if randomAction:\n",
    "                p = np.where(b.b==0, 1, 0)\n",
    "                policy = p / np.sum(p)\n",
    "                x,y = chooseMove(policy)\n",
    "                randomAction = False\n",
    "            else:\n",
    "                x,y = policyMCMove(b, q, player)\n",
    "            \n",
    "            listStateAction.append((key, (x, y)))\n",
    "        \n",
    "        gameOver = b.move(x, y, player)\n",
    "        \n",
    "        player *= -1\n",
    "        \n",
    "    if gameOver == 2:\n",
    "        r = 0.5\n",
    "    elif gameOver == 1:\n",
    "        r = 0\n",
    "    else:\n",
    "        r = 1\n",
    "    \n",
    "    for key, (x,y) in listStateAction:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        \n",
    "        u = (key, a)\n",
    "        \n",
    "        q[u] = q[u] * n[u] + r\n",
    "        n[u] += 1\n",
    "        q[u] /= n[u]\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, c = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500000/500000 [03:09<00:00, 2641.04it/s]\n",
      "100%|██████████| 500000/500000 [02:02<00:00, 4065.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05966 0.903212 0.037128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#off policy\n",
    "nGames = 500000\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    listStateAction=[]\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x,y=policyRandomMove(b, True)\n",
    "            listStateAction.append((b.toKey(), (x, y)))\n",
    "        gameOver = b.move(x,y,player)\n",
    "        player=-player\n",
    "        \n",
    "    if gameOver==2:\n",
    "        r=0.5\n",
    "    elif gameOver==1:\n",
    "        r=0\n",
    "    else:\n",
    "        r=1\n",
    "\n",
    "    W=1\n",
    "    for k, (x,y) in listStateAction[::]:\n",
    "        a = Board.coordToLinearCoord(x, y)\n",
    "        c[(k,a)]=c[(k,a)]+W\n",
    "        q[(k,a)]=q[(k,a)]+W*(r-q[(k,a)])/c[(k,a)]\n",
    "        b = Board(np.zeros((3,3)))\n",
    "        b.fromKey(k)\n",
    "        pA=b.possibleActions()\n",
    "        bestA=pA[0]\n",
    "        for aa in pA[1:]:\n",
    "            if q[(k,aa)]>q[(k,bestA)]:\n",
    "                bestA=aa\n",
    "        if bestA!=a:\n",
    "            break\n",
    "        W=W*len(pA)\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:37<00:00, 2670.81it/s]\n",
      "100%|██████████| 100000/100000 [00:36<00:00, 2746.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.04545 0.76722 0.18733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#sarsa\n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "epsilon = 0.1\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,epsilon)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = x+3*y\n",
    "                newKey = b.toKey()\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif player==-1 and newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(q[(newKey,newAction)]-q[(key,action)])\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:39<00:00, 2530.80it/s]\n",
      "100%|██████████| 100000/100000 [00:33<00:00, 2961.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03501 0.75657 0.20842\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#q learning\n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "epsilon=0.1\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,epsilon)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = x+3*y\n",
    "                newKey = b.toKey()\n",
    "                pA=b.possibleActions()\n",
    "                maxq=q[(newKey,pA[0])]\n",
    "                for aa in pA[1:]:\n",
    "                    if q[(newKey,aa)]>maxq:\n",
    "                        maxq=q[(newKey,aa)]\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif player==-1 and newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(maxq-q[(key,action)])\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [00:41<00:00, 2411.82it/s]\n",
      "100%|██████████| 100000/100000 [00:37<00:00, 2659.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02518 0.77884 0.19598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# expected sarsa where behaviour policy is eps greedy derived from q\n",
    "# (expected sarsa where behaviour policy is greedy derived from q is the same as q learning)\n",
    "nGames = 100000\n",
    "alpha = 0.1\n",
    "epsilon = 0.1\n",
    "\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    \n",
    "    player = random.choice([-1,1])\n",
    "    \n",
    "    gameOver = 0\n",
    "    \n",
    "    action = None\n",
    "    newAction = None\n",
    "    \n",
    "    while gameOver == 0:\n",
    "        if player == 1:\n",
    "            x, y = policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b, q, player, epsilon)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = Board.coordToLinearCoord(x, y)\n",
    "                newKey = b.toKey()\n",
    "                expectedTarget = expectedTargetUnderEpsGreedy(b, q, epsilon)\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "        elif player==-1 and newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(expectedTarget-q[(key,action)])\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learning improves a policy from experience gathered by interacting with the environment (direct RL). During this learning, a model of the environment can be created by remembering the rewards and new states that followed states and actions. Such a model can be used for planning (indirect RL). Direct and indirect RL are combined in the dyna architectures. If the environment is deterministic, then the model can also be deterministic and sample updates can be used during the planning step. If the environmenet is stochastic, then the model should also be stochastic and normally expected updates are used.\n",
    "\n",
    "Planning is used to make the learning more sample efficient.\n",
    "\n",
    "During planning, we have to choose states for the planning step. this state search can be uniformly random (like in dyna) or follow rules, eg prioritized sweeping.\n",
    "\n",
    "We implement dyna-q as an example. prioritized sweeping seems exaggerated for these short episode with few actions.\n",
    "We couldnt observe a great improvement in sample efficiency which might be due to the opponent following a random strategy which makes the environment (almost pure) random.\n",
    "\n",
    "There is another way of planning, sometimes called planning at decision time where the planning focuses on the current state, eg Monte Carlo Tree Search. In this simple example of TicTacToe, such algorithms can compute the optimal fast enough such that they would be a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "q, _ = initialise(getAllStartStates())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100000/100000 [07:12<00:00, 231.17it/s]\n",
      "100%|██████████| 100000/100000 [00:25<00:00, 3955.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10859 0.87181 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#dyna-q (for stoch env)\n",
    "\n",
    "def updateModel(model, key, newState):\n",
    "    if key in model:\n",
    "        dist=model[key]\n",
    "        if newState in dist:\n",
    "            dist[newState]+=1\n",
    "        else:\n",
    "            dist[newState]=1\n",
    "    else:\n",
    "        dist={newState:1}\n",
    "        model[key]=dist\n",
    "    \n",
    "nGames = 100000\n",
    "alpha=0.1\n",
    "epsilon=0.1\n",
    "model={}\n",
    "hb = Board(np.zeros((3,3)))\n",
    "nPlanSteps=20\n",
    "for _ in tqdm(range(nGames)):\n",
    "    b = Board(np.zeros((3,3)))\n",
    "    player = random.choice([-1,1])\n",
    "    gameOver=0\n",
    "    action=None\n",
    "    newAction=None\n",
    "    while gameOver==0:\n",
    "        if player==1:\n",
    "            x,y=policyRandomMove(b)\n",
    "        else:\n",
    "            x, y = policyMCMoveEpsGreedy(b,q,player,epsilon)\n",
    "            if action is None:\n",
    "                action = x+3*y\n",
    "                key = b.toKey()\n",
    "            else:\n",
    "                newAction = x+3*y\n",
    "                newKey = b.toKey()\n",
    "                pA=b.possibleActions()\n",
    "                maxq=q[(newKey,pA[0])]\n",
    "                for aa in pA[1:]:\n",
    "                    if q[(newKey,aa)]>maxq:\n",
    "                        maxq=q[(newKey,aa)]\n",
    "        gameOver = b.move(x,y,player)\n",
    "        if gameOver==-1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(1-q[(key,action)])\n",
    "            updateModel(model, (key, action), newKey)\n",
    "            updateModel(model, (newKey, newAction), -b.toKey())\n",
    "        elif gameOver==1:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(0-q[(key,action)])\n",
    "            updateModel(model, (key, action), -b.toKey())\n",
    "        elif player==-1 and newAction is not None:\n",
    "            q[(key,action)]=q[(key,action)]+alpha*(maxq-q[(key,action)])\n",
    "            updateModel(model, (key, action), newKey)\n",
    "            action=newAction\n",
    "            key=newKey\n",
    "\n",
    "        if player==-1 and newAction is not None:\n",
    "            for _ in range(nPlanSteps):\n",
    "                visitedStates = list(model.keys())\n",
    "                k = visitedStates[np.random.randint(len(visitedStates))]\n",
    "                dist = model[k]\n",
    "                d = 0\n",
    "                for s in dist:\n",
    "                    d+=dist[s]\n",
    "                u = np.random.uniform()\n",
    "                d2=0\n",
    "                for s in dist:\n",
    "                    if u < d2+dist[s]/d:\n",
    "                        state=s\n",
    "                    else:\n",
    "                        d2+=dist[s]/d\n",
    "                if state<0:\n",
    "                    r = 0\n",
    "                    hb.fromKey(-state)\n",
    "                    if hb.check(-1):\n",
    "                        r = 1\n",
    "                    state = -state\n",
    "                else:\n",
    "                    hb.fromKey(state)\n",
    "                    pA=hb.possibleActions()\n",
    "                    maxq=q[(state,pA[0])]\n",
    "                    for aa in pA[1:]:\n",
    "                        if q[(state,aa)]>maxq:\n",
    "                            maxq=q[(state,aa)]\n",
    "                    r = maxq\n",
    "                q[k]=q[k]+alpha*(r-q[k])\n",
    "            \n",
    "        player=-player\n",
    "\n",
    "playGame(nGames, q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
